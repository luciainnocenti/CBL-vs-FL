{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYU5T089Xo64"
      },
      "outputs": [],
      "source": [
        "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
        "from fedbiomed.common.constants import ErrorNumbers\n",
        "from fedbiomed.common.exceptions import FedbiomedStrategyError\n",
        "from fedbiomed.common.logger import logger\n",
        "\n",
        "\n",
        "class MedicalFolderStrategy(DefaultStrategy):\n",
        "    def __init__(self, data, modalities=['T1']):\n",
        "        super().__init__(data)\n",
        "        self._modalities = modalities\n",
        "\n",
        "    def refine(self, training_replies, round_i):\n",
        "        models_params = []\n",
        "        weights = []\n",
        "\n",
        "        # check that all nodes answered\n",
        "        cl_answered = [val['node_id'] for val in training_replies.data()]\n",
        "\n",
        "        answers_count = 0\n",
        "        for cl in self.sample_nodes(round_i):\n",
        "            if cl in cl_answered:\n",
        "                answers_count += 1\n",
        "            else:\n",
        "                # this node did not answer\n",
        "                logger.error(f'{ErrorNumbers.FB408.value} (node = {cl})')\n",
        "\n",
        "        if len(self.sample_nodes(round_i)) != answers_count:\n",
        "            if answers_count == 0:\n",
        "                # none of the nodes answered\n",
        "                msg = ErrorNumbers.FB407.value\n",
        "\n",
        "            else:\n",
        "                msg = ErrorNumbers.FB408.value\n",
        "\n",
        "            logger.critical(msg)\n",
        "            raise FedbiomedStrategyError(msg)\n",
        "\n",
        "        # check that all nodes that answer could successfully train\n",
        "        self._success_node_history[round_i] = []\n",
        "        all_success = True\n",
        "        for tr in training_replies:\n",
        "            if tr['success'] is True:\n",
        "                model_params = {tr['node_id']: tr['params']}\n",
        "                models_params.append(model_params)\n",
        "                self._success_node_history[round_i].append(tr['node_id'])\n",
        "            else:\n",
        "                # node did not succeed\n",
        "                all_success = False\n",
        "                logger.error(f'{ErrorNumbers.FB409.value} (node = {tr[\"node_id\"]})')\n",
        "\n",
        "        if not all_success:\n",
        "            raise FedbiomedStrategyError(ErrorNumbers.FB402.value)\n",
        "\n",
        "        # so far, everything is OK\n",
        "        shapes = [val[0][\"shape\"]['demographics'][0] for (key, val) in self._fds.data().items()]\n",
        "        total_rows = sum(shapes)\n",
        "        weights = [{key: val[0][\"shape\"]['demographics'][0] / total_rows} for (key, val) in self._fds.data().items()]\n",
        "        logger.info('Nodes that successfully reply in round ' +\n",
        "            str(round_i) + ' ' +\n",
        "            str(self._success_node_history[round_i]))\n",
        "        return models_params, weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
        "from fedbiomed.common.data import DataManager, MedicalFolderDataset\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import AdamW, SGD\n",
        "from monai.losses.dice import DiceLoss\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import Compose, SpatialPad, CenterSpatialCrop, NormalizeIntensity, AddChannel, AsDiscrete, Lambda\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class UNetTrainingPlan(TorchTrainingPlan):\n",
        "    # Init of UNetTrainingPlan\n",
        "\n",
        "    def init_model(self, model_args):\n",
        "        model = self.Net(model_args)\n",
        "        return model\n",
        "\n",
        "    def init_optimizer(self, optimizer_args):\n",
        "        tmp_args = self.model().model_arguments\n",
        "        optimizer_name = tmp_args['optimizer_name'] if 'optimizer_name' in tmp_args.keys() else 'adam'\n",
        "        if optimizer_name == 'adam':\n",
        "            optimizer = AdamW(self.model().parameters(), **optimizer_args)\n",
        "        elif optimizer_name == 'sgd':\n",
        "            optimizer = torch.optim.SGD(self.model().parameters(), **optimizer_args)\n",
        "        else:\n",
        "            return -1\n",
        "        return optimizer\n",
        "\n",
        "    def init_dependencies(self):\n",
        "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
        "        deps = [\n",
        "            \"from monai.transforms import Compose, SpatialPad, CenterSpatialCrop, NormalizeIntensity, \"\n",
        "            \"AddChannel, Resize, AsDiscrete, Lambda\",\n",
        "            \"import torch.nn as nn\",\n",
        "            \"import torch\",\n",
        "            \"from monai.losses.dice import DiceLoss\",\n",
        "            \"import torch.nn.functional as F\",\n",
        "            \"from fedbiomed.common.data import MedicalFolderDataset\",\n",
        "            \"import numpy as np\",\n",
        "            \"from torch.optim import AdamW\",\n",
        "            \"from monai.networks.nets import UNet\"\n",
        "        ]\n",
        "        return deps\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        # Init of UNetTrainingPlan\n",
        "        def __init__(self, model_args: dict = {}):\n",
        "            super().__init__()\n",
        "            self.CHANNELS_DIMENSION = 1\n",
        "            self.unet = UNet(\n",
        "                spatial_dims=3,\n",
        "                in_channels=1,\n",
        "                out_channels=2,\n",
        "                channels=(16, 32, 64, 128, 256),\n",
        "                strides=(2, 2, 2, 2),\n",
        "                num_res_units=model_args['num_res_units'],\n",
        "                norm=\"batch\",\n",
        "                dropout=model_args['dropout'],\n",
        "            )\n",
        "            self.model_arguments = model_args\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.unet.forward(x)\n",
        "            x = F.softmax(x, dim=self.CHANNELS_DIMENSION)\n",
        "            return x\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dice_loss(output, target):\n",
        "        loss = DiceLoss(include_background=False, sigmoid=False)\n",
        "        loss_value = loss(output, target)\n",
        "        return loss_value\n",
        "\n",
        "    @staticmethod\n",
        "    def demographics_transform(demographics):\n",
        "        if isinstance(demographics, dict) and len(demographics) == 0:\n",
        "            # when input is empty dict, we don't want to transform anything\n",
        "            return demographics\n",
        "\n",
        "        # simple example: keep only some keys\n",
        "        out = np.array([float(val) for key, val in demographics.items()])\n",
        "        return out\n",
        "\n",
        "    def training_data(self, batch_size=4):\n",
        "        # The training_data creates the Dataloader to be used for training in the general class Torchnn of fedbiomed\n",
        "        common_shape = (320, 320, 16)\n",
        "\n",
        "        training_transform = Compose([AddChannel(), CenterSpatialCrop(common_shape),\n",
        "                                      SpatialPad(common_shape), NormalizeIntensity()])\n",
        "        target_transform = Compose([AddChannel(), CenterSpatialCrop(common_shape), SpatialPad(common_shape),\n",
        "                                    Lambda(func=lambda x: torch.where(x != 0, 1, 0)),\n",
        "                                    AsDiscrete(to_onehot=2)\n",
        "                                    ])\n",
        "\n",
        "        dataset = MedicalFolderDataset(\n",
        "            root=self.dataset_path,\n",
        "            data_modalities='image',\n",
        "            target_modalities='label',\n",
        "            transform=training_transform,\n",
        "            target_transform=target_transform,\n",
        "            demographics_transform=UNetTrainingPlan.demographics_transform)\n",
        "        train_kwargs = {'batch_size': batch_size, 'shuffle': True}\n",
        "        return DataManager(dataset, **train_kwargs)\n",
        "\n",
        "    def training_step(self, data, target):\n",
        "        # this function must return the loss to backward it\n",
        "        img = data[0]['image']\n",
        "        target = target['label']\n",
        "        output = self.model().forward(img)\n",
        "        loss = UNetTrainingPlan.get_dice_loss(output, target)\n",
        "        avg_loss = loss.mean()\n",
        "        return avg_loss\n",
        "\n",
        "    def testing_step(self, data, target):\n",
        "        img = data[0]['image']\n",
        "        target = target['label']\n",
        "        prediction = self.model().forward(img)\n",
        "        loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n",
        "        avg_loss = loss.mean()  # average per batch\n",
        "        return avg_loss\n"
      ],
      "metadata": {
        "id": "7nH_R_9-YZh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_rounds(configuration: str, config_args: dict):\n",
        "    num_updates = int(config_args['num_updates']) if 'num_updates' in config_args.keys() else 20\n",
        "    num_epochs_local = int(config_args['num_epochs_local']) if 'num_epochs_local' in config_args.keys() else 100\n",
        "    num_clients = int(config_args['num_clients']) if 'num_clients' in config_args.keys() else 4\n",
        "    batch_size = int(config_args['batch_size']) if 'batch_size' in config_args.keys() else 8\n",
        "    dim_training_set = 210 if num_clients == 3 else 230\n",
        "    if configuration in ['LocalNode', 'centralized']:\n",
        "        return 1\n",
        "    else:\n",
        "        return (dim_training_set // num_clients // batch_size) * num_epochs_local // num_updates\n",
        "\n",
        "\n",
        "def get_training_size(dataset: str):\n",
        "    DS_SIZES = {'decathlon': 32,\n",
        "                'promise_no_coil': 23,\n",
        "                'promise_coil': 27,\n",
        "                'prostatex_skyra': 174\n",
        "                }\n",
        "    return DS_SIZES[dataset]\n"
      ],
      "metadata": {
        "id": "tXTLTU8HYf6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_reference = \"kfold_0\"\n",
        "configuration = \"LocalNode\" #or FedAvg, FedProx, Yogi, FedAdam, FedAdagrad, Centralized\n",
        "configuration_file = \"config.ini\"\n",
        "config = configparser.ConfigParser()\n",
        "config.read(configuration_file)\n",
        "config_args = dict(config.items(configuration))\n",
        "config_args['configuration'] = configuration"
      ],
      "metadata": {
        "id": "sSmBg6fsYwIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = {\n",
        "    'use_gpu': True,\n",
        "    'dropout': float(config_args['dropout']) if 'dropout' in config_args.keys() else 0,\n",
        "    'num_res_units': int(config_args['num_res_units']),\n",
        "    'optimizer_name': config_args['optimizer_name'],\n",
        "\n",
        "}\n",
        "\n",
        "if 'num_upgrades_local_run' in config_args.keys():\n",
        "    local_dim = get_training_size(config_args['used_datasets'])\n",
        "    training_args['epochs'] = int(\n",
        "        int(config_args['num_upgrades_local_run']) * training_args['batch_size'] / local_dim)\n",
        "elif 'epochs' in config_args.keys():\n",
        "    training_args['epochs'] = int(config_args['epochs'])\n",
        "\n",
        "if configuration == 'scaffold':\n",
        "    aggregator = Scaffold(server_lr=1)\n",
        "else:\n",
        "    aggregator = FedAverage()\n",
        "    if configuration == 'FedProx':\n",
        "        training_args['fedprox_mu'] = float(config_args['fedprox_mu'])\n",
        "if config_args['optimizer_name'] == 'sgd':\n",
        "    training_args['optimizer_args']['momentum'] = float(config_args['momentum']) \\\n",
        "        if 'momentum' in config_args.keys() else 0.\n",
        "num_rounds = get_num_rounds(configuration, config_args)\n",
        "config_args['rounds'] = num_rounds\n",
        "if \"dp\" in config_args.keys() and eval(config_args['dp']) is True:\n",
        "    sigma = 4.\n",
        "    clip = 1.\n",
        "    LDP = {'dp_args': {'type': 'local', 'sigma': sigma, 'clip': clip}}\n",
        "    model_args.update(LDP)"
      ],
      "metadata": {
        "id": "sbuqbLKPZApu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = Experiment(tags=tags,\n",
        "                  model_args=model_args,\n",
        "                  training_plan_class=UNetTrainingPlan,\n",
        "                  training_args=training_args,\n",
        "                  round_limit=num_rounds,\n",
        "                  aggregator=aggregator,\n",
        "                  tensorboard=False, # or True\n",
        "                  save_breakpoints=False) # or True\n",
        "if configuration == 'Yogi':\n",
        "    exp.set_agg_optimizer(Optimizer(lr=.9, modules=[YogiModule()]))\n",
        "elif configuration == 'FedAdagrad':\n",
        "    exp.set_agg_optimizer(Optimizer(lr=.9, modules=[AdaGradModule()]))\n",
        "elif configuration == 'FedAdam':\n",
        "    exp.set_agg_optimizer(Optimizer(lr=.9, modules=[AdamModule()]))"
      ],
      "metadata": {
        "id": "553g1m5bZM42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt1 = datetime.now(tz=pytz.timezone('Europe/Rome'))\n",
        "exp.run()\n",
        "dt2 = datetime.now(tz=pytz.timezone('Europe/Rome'))\n",
        "config_args['training_time'] = (dt2 - dt1).total_seconds()\n",
        "config_args['exp.training_plan_file()'] = exp.training_plan_file(display=False)\n",
        "try:\n",
        "    model = exp.model_instance()\n",
        "except AttributeError:\n",
        "    model = exp.training_plan().model()\n",
        "except:\n",
        "    print('Model not find')\n",
        "model.load_state_dict(exp.aggregated_params()[num_rounds - 1]['params'])\n",
        "config_args['n_parameters'] = sum(param.numel() for param in model.parameters())\n",
        "config_args['n_trainable_parameters'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Saving model...\")\n",
        "\n",
        "if \"dp\" in config_args.keys() and eval(config_args['dp']) is True:\n",
        "    results_folder = f\"{config_args['results_folder']}/{id_reference}_dp\"\n",
        "else:\n",
        "    results_folder = f\"{config_args['results_folder']}/{id_reference}\"\n",
        "\n",
        "folder_name = \"\"\n",
        "\n",
        "if os.path.exists(os.path.join(config_args['results_folder'], id_reference, folder_name)):\n",
        "    shutil.rmtree(os.path.join(config_args['results_folder'], id_reference, folder_name))\n",
        "os.makedirs(os.path.join(config_args['results_folder'], id_reference, folder_name))\n",
        "\n",
        "torch.save(model.unet.state_dict(), f'{results_folder}/{folder_name}/unet')\n"
      ],
      "metadata": {
        "id": "9nCmnetWZYsF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}