{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXGJKbJLciJq"
      },
      "outputs": [],
      "source": [
        "from fedbiomed.researcher.strategies.default_strategy import DefaultStrategy\n",
        "from fedbiomed.common.constants import ErrorNumbers\n",
        "from fedbiomed.common.exceptions import FedbiomedStrategyError\n",
        "from fedbiomed.common.logger import logger\n",
        "\n",
        "\n",
        "class MedicalFolderStrategy(DefaultStrategy):\n",
        "    def __init__(self, data, modalities=['T1']):\n",
        "        super().__init__(data)\n",
        "        self._modalities = modalities\n",
        "\n",
        "    def refine(self, training_replies, round_i):\n",
        "        models_params = []\n",
        "        weights = []\n",
        "\n",
        "        # check that all nodes answered\n",
        "        cl_answered = [val['node_id'] for val in training_replies.data()]\n",
        "\n",
        "        answers_count = 0\n",
        "        for cl in self.sample_nodes(round_i):\n",
        "            if cl in cl_answered:\n",
        "                answers_count += 1\n",
        "            else:\n",
        "                # this node did not answer\n",
        "                logger.error(f'{ErrorNumbers.FB408.value} (node = {cl})')\n",
        "\n",
        "        if len(self.sample_nodes(round_i)) != answers_count:\n",
        "            if answers_count == 0:\n",
        "                # none of the nodes answered\n",
        "                msg = ErrorNumbers.FB407.value\n",
        "\n",
        "            else:\n",
        "                msg = ErrorNumbers.FB408.value\n",
        "\n",
        "            logger.critical(msg)\n",
        "            raise FedbiomedStrategyError(msg)\n",
        "\n",
        "        # check that all nodes that answer could successfully train\n",
        "        self._success_node_history[round_i] = []\n",
        "        all_success = True\n",
        "        for tr in training_replies:\n",
        "            if tr['success'] is True:\n",
        "                model_params = {tr['node_id']: tr['params']}\n",
        "                models_params.append(model_params)\n",
        "                self._success_node_history[round_i].append(tr['node_id'])\n",
        "            else:\n",
        "                # node did not succeed\n",
        "                all_success = False\n",
        "                logger.error(f'{ErrorNumbers.FB409.value} (node = {tr[\"node_id\"]})')\n",
        "\n",
        "        if not all_success:\n",
        "            raise FedbiomedStrategyError(ErrorNumbers.FB402.value)\n",
        "\n",
        "        # so far, everything is OK\n",
        "        shapes = [val[0][\"shape\"]['demographics'][0] for (key, val) in self._fds.data().items()]\n",
        "        total_rows = sum(shapes)\n",
        "        weights = [{key: val[0][\"shape\"]['demographics'][0] / total_rows} for (key, val) in self._fds.data().items()]\n",
        "        logger.info('Nodes that successfully reply in round ' +\n",
        "            str(round_i) + ' ' +\n",
        "            str(self._success_node_history[round_i]))\n",
        "        return models_params, weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from fedbiomed.common.training_plans import TorchTrainingPlan\n",
        "from fedbiomed.common.data import DataManager, MedicalFolderDataset\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam, SGD\n",
        "from monai.losses.dice import DiceLoss\n",
        "from monai.networks.nets import UNet, SegResNet\n",
        "from monai.transforms import Compose, Resize, NormalizeIntensity, AddChannel, AsDiscrete, Lambda, CenterSpatialCrop\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class UNetTrainingPlan(TorchTrainingPlan):\n",
        "\n",
        "    def init_model(self, model_args):\n",
        "        model = self.Net(model_args)\n",
        "        return model\n",
        "\n",
        "    def init_optimizer(self, optimizer_args):\n",
        "        tmp_args = self.model().model_arguments\n",
        "        optimizer_name = tmp_args['optimizer_name'] if 'optimizer_name' in tmp_args.keys() else 'adam'\n",
        "        if optimizer_name == 'adam':\n",
        "            optimizer = Adam(self.model().parameters(), **optimizer_args)\n",
        "        elif optimizer_name == 'sgd':\n",
        "            optimizer = SGD(self.model().parameters(), **optimizer_args)\n",
        "        # optimizer = Adam(self.model().parameters(), lr=optimizer_args['lr'])\n",
        "        return optimizer\n",
        "\n",
        "    def init_dependencies(self):\n",
        "        # Here we define the custom dependencies that will be needed by our custom Dataloader\n",
        "        deps = [\"from monai.transforms import (Compose, NormalizeIntensity, AddChannel, CenterSpatialCrop, \"\n",
        "                \"AsDiscrete, Lambda)\",\n",
        "                \"import torch.nn as nn\",\n",
        "                'import torch.nn.functional as F',\n",
        "                \"from fedbiomed.common.data import MedicalFolderDataset\",\n",
        "                'import numpy as np',\n",
        "                'from monai.losses.dice import DiceLoss',\n",
        "                'from torch.optim import AdamW, Adam, SGD',\n",
        "                \"from monai.networks.nets import UNet, SegResNet\"]\n",
        "        return deps\n",
        "\n",
        "    class Net(nn.Module):\n",
        "        # Init of UNetTrainingPlan\n",
        "        def __init__(self, model_args: dict = {}):\n",
        "            super().__init__()\n",
        "            self.CHANNELS_DIMENSION = 1\n",
        "            if model_args['network_type'] == 'unet':\n",
        "                print(f\"Selected model is unet\")\n",
        "                net = UNet(\n",
        "                    spatial_dims=3,\n",
        "                    in_channels=4,\n",
        "                    out_channels=4,\n",
        "                    channels=(30, 30 * 2, 30 * 4, 30 * 8, 30 * 16),\n",
        "                    strides=(2, 2, 2, 2),\n",
        "                    num_res_units=2,\n",
        "                    kernel_size=3,\n",
        "                    dropout=0.3,\n",
        "                )\n",
        "            elif model_args['network_type'] == 'segresnet':\n",
        "                print(f\"Selected model is segresnet\")\n",
        "                net = SegResNet(spatial_dims=3,\n",
        "                                init_filters=16,\n",
        "                                in_channels=4,\n",
        "                                out_channels=4,\n",
        "                                dropout_prob=0.2,\n",
        "                                act=('RELU', {'inplace': True}),\n",
        "                                norm=('GROUP', {'num_groups': 8}),\n",
        "                                norm_name='',\n",
        "                                num_groups=8,\n",
        "                                use_conv_final=True,\n",
        "                                blocks_down=(1, 2, 2, 4),\n",
        "                                blocks_up=(1, 1, 1)\n",
        "                                )\n",
        "            else:\n",
        "                raise ValueError(f'The mode_args dictionary contains {model_args[\"network_type\"]}, '\n",
        "                                 f'which is not an allowed value')\n",
        "            self.net = net\n",
        "            self.model_arguments = model_args\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.net.forward(x)\n",
        "            x = F.softmax(x, dim=self.CHANNELS_DIMENSION)\n",
        "            return x\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dice_loss(output, target):\n",
        "        loss = DiceLoss(include_background=True, sigmoid=False)\n",
        "        loss_value = loss(output, target)\n",
        "        return loss_value\n",
        "\n",
        "    @staticmethod\n",
        "    def demographics_transform(demographics: dict):\n",
        "        return {}\n",
        "\n",
        "    def training_data(self, batch_size=4):\n",
        "        # The training_data creates the Dataloader to be used for training in the general class Torchnn of fedbiomed\n",
        "        common_shape = (240, 240, 128)\n",
        "        training_transform = {modality: Compose([AddChannel(), CenterSpatialCrop(common_shape), NormalizeIntensity(), ]) for\n",
        "                              modality in ('T1', 'T1CE', 'FLAIR', 'T2')}\n",
        "        target_transform = Compose([AddChannel(), CenterSpatialCrop(common_shape),\n",
        "                                    Lambda(func=lambda x: torch.where(x == 4, 3, x)),\n",
        "                                    AsDiscrete(to_onehot=4)])\n",
        "\n",
        "        dataset = MedicalFolderDataset(\n",
        "            root=self.dataset_path,\n",
        "            data_modalities=['T1', 'T1CE', 'FLAIR', 'T2'],\n",
        "            target_modalities='SEG',\n",
        "            transform=training_transform,\n",
        "            target_transform=target_transform,\n",
        "            demographics_transform=UNetTrainingPlan.demographics_transform)\n",
        "        loader_arguments = {'batch_size': batch_size, 'shuffle': False}\n",
        "        return DataManager(dataset, **loader_arguments)\n",
        "\n",
        "    def training_step(self, data, target):\n",
        "        torch.cuda.empty_cache()\n",
        "        # this function must return the loss to backward it\n",
        "        img = torch.cat((data[0]['T1CE'], data[0]['T1'], data[0]['T2'], data[0]['FLAIR']), 1)\n",
        "        demographics = data[1]\n",
        "        output = self.model().forward(img)\n",
        "        loss = UNetTrainingPlan.get_dice_loss(output, target['SEG'])\n",
        "        avg_loss = loss.mean()\n",
        "        return avg_loss\n",
        "\n",
        "    def testing_step(self, data, target):\n",
        "        torch.cuda.empty_cache()\n",
        "        img = torch.cat((data[0]['T1CE'], data[0]['T1'], data[0]['T2'], data[0]['FLAIR']), 1)\n",
        "        demographics = data[1]\n",
        "        target = target['SEG']\n",
        "        prediction = self.model().forward(img)\n",
        "        loss = UNetTrainingPlan.get_dice_loss(prediction, target)\n",
        "        avg_loss = loss.item().mean()  # average per batch\n",
        "        return avg_loss\n"
      ],
      "metadata": {
        "id": "FNmwuZo8daLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import configparser\n",
        "from mmap import mmap\n",
        "\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "from fedbiomed.researcher.experiment import Experiment\n",
        "from fedbiomed.researcher.aggregators.fedavg import FedAverage\n",
        "from fedbiomed.researcher.aggregators.scaffold import Scaffold\n",
        "from fedbiomed.researcher.environ import environ\n",
        "from fedbiomed.common.logger import logger\n",
        "from declearn.optimizer.modules import AdamModule, AdaGradModule, YogiModule\n",
        "from declearn.optimizer.regularizers import FedProxRegularizer\n",
        "from fedbiomed.common.optimizers.optimizer import Optimizer\n",
        "from plan_segmentation import UNetTrainingPlan\n",
        "import torch\n",
        "import pickle\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "HOME = str(Path.home())\n",
        "PROJECT_DIR = f'{HOME}/fets_analysis'\n",
        "\n",
        "\n",
        "def get_num_rounds(configuration: str, config_args: dict):\n",
        "    num_updates = int(config_args['num_updates']) if 'num_updates' in config_args.keys() else 20\n",
        "    num_epochs_local = int(config_args['num_epochs_local']) if 'num_epochs_local' in config_args.keys() else 25\n",
        "    num_clients = int(config_args['num_clients']) if 'num_clients' in config_args.keys() else 23\n",
        "    batch_size = int(config_args['batch_size']) if 'batch_size' in config_args.keys() else 8\n",
        "    dim_training_set = 1000\n",
        "    if configuration in ['LocalNode', 'centralized']:\n",
        "        return 1\n",
        "    else:\n",
        "        return (dim_training_set // num_clients // batch_size) * num_epochs_local // num_updates\n",
        "\n",
        "\n",
        "def mapcount(filename):\n",
        "    f = open(filename, \"r+\")\n",
        "    buf = mmap(f.fileno(), 0)\n",
        "    lines = 0\n",
        "    readline = buf.readline\n",
        "    while readline():\n",
        "        lines += 1\n",
        "    return lines\n",
        "\n",
        "\n",
        "def get_training_size(dataset: str):\n",
        "    file_csv = os.path.join(PROJECT_DIR, 'splits', dataset, 'participants_train_kfold_0.csv')\n",
        "    return mapcount(file_csv)\n"
      ],
      "metadata": {
        "id": "m5buSeDWdhmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_reference = \"kfold_0\"\n",
        "configuration = \"LocalNode\" #or FedAvg, FedProx, Yogi, FedAdam, FedAdagrad, Centralized\n",
        "configuration_file = \"config.ini\"\n",
        "config = configparser.ConfigParser()\n",
        "config.read(configuration_file)\n",
        "config_args = dict(config.items(configuration))\n",
        "config_args['configuration'] = configuration\n",
        "\n",
        "tags = [config_args['tags'], id_reference]\n",
        "FEDBIOMED_DIR = \"\"\n",
        "if configuration == \"LocalNode\":\n",
        "    tags.append('Local0') # or local name, has to match the tag used in the node\n"
      ],
      "metadata": {
        "id": "whbOCQHpeEqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = {'batch_size': int(config_args['batch_size']) if 'batch_size' in config_args.keys() else 8,\n",
        "                  'dry_run': False,\n",
        "                  'log_interval': int(config_args['log_interval']) if 'log_interval' in config_args.keys() else 3,\n",
        "                  'test_ratio': float(config_args['test_ratio']) if 'test_ratio' in config_args.keys() else 0.1,\n",
        "                  'test_on_global_updates': False, 'test_on_local_updates': False,\n",
        "                  'num_updates': int(config_args['num_updates']) if 'num_updates' in config_args.keys() else None,\n",
        "                  'optimizer_args': {\n",
        "                      'lr': float(config_args['lr']) if 'lr' in config_args.keys() else 0.001,\n",
        "                  },\n",
        "                  'use_gpu': True if torch.cuda.is_available() else False,\n",
        "                  }\n",
        "\n",
        "model_args = {\n",
        "    'use_gpu': True if torch.cuda.is_available() else False,\n",
        "    'dropout': float(config_args['dropout']) if 'dropout' in config_args.keys() else 0,\n",
        "    'optimizer_name': config_args['optimizer_name'],\n",
        "    'network_type': 'segresnet'\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "tC4_i1r9ep88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if 'num_upgrades_local_run' in config_args.keys():\n",
        "    local_dim = get_training_size(config_args['used_datasets'])\n",
        "    training_args['epochs'] = int(\n",
        "        int(config_args['num_upgrades_local_run']) * training_args['batch_size'] / local_dim) + 1\n",
        "elif 'epochs' in config_args.keys():\n",
        "    training_args['epochs'] = int(config_args['epochs'])\n",
        "\n",
        "if configuration == 'scaffold':\n",
        "    aggregator = Scaffold(server_lr=1)\n",
        "\n",
        "else:\n",
        "    aggregator = FedAverage()\n",
        "    if configuration == 'FedProx':\n",
        "        training_args['fedprox_mu'] = float(config_args['fedprox_mu'])\n",
        "if config_args['optimizer_name'] == 'sgd':\n",
        "    training_args['optimizer_args']['momentum'] = float(config_args['momentum']) \\\n",
        "        if 'momentum' in config_args.keys() else 0.\n",
        "num_rounds = get_num_rounds(configuration, config_args)\n",
        "config_args['rounds'] = num_rounds"
      ],
      "metadata": {
        "id": "u3ua0Po0e5l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = Experiment(tags=tags,\n",
        "                  model_args=model_args,\n",
        "                  training_plan_class=UNetTrainingPlan,\n",
        "                  training_args=training_args,\n",
        "                  round_limit=num_rounds,\n",
        "                  aggregator=aggregator,\n",
        "                  tensorboard=False, # or True\n",
        "                  save_breakpoints=False, #or True\n",
        "                  )\n",
        "if model_args['network_type'] == 'segresnet':\n",
        "    logger.info('loading pre-trained model')\n",
        "    model_params = torch.load(f\"{PROJECT_DIR}/pretraining_params.pt\", map_location=torch.device('cpu'))\n",
        "    exp.training_plan().set_model_params(model_params)\n",
        "    exp._job.update_parameters()"
      ],
      "metadata": {
        "id": "CzfTeVHEfGse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if configuration == 'FedYogi':\n",
        "    exp.set_agg_optimizer(Optimizer(lr=.9, modules=[YogiModule()]))\n",
        "elif configuration == 'FedAdagrad':\n",
        "    exp.set_agg_optimizer(Optimizer(lr=.9, modules=[AdaGradModule()]))\n",
        "elif configuration == 'FedAdam':\n",
        "    exp.set_agg_optimizer(Optimizer(lr=.9, modules=[AdamModule()]))"
      ],
      "metadata": {
        "id": "ohFuc5F_fRgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt1 = datetime.now(tz=pytz.timezone('Europe/Rome'))\n",
        "exp.run()\n",
        "dt2 = datetime.now(tz=pytz.timezone('Europe/Rome'))\n",
        "config_args['training_time'] = (dt2 - dt1).total_seconds()"
      ],
      "metadata": {
        "id": "_n5XEi8_fSkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(exp.aggregated_params()[num_rounds - 1]['params'])\n",
        "config_args['n_parameters'] = sum(param.numel() for param in model.parameters())\n",
        "config_args['n_trainable_parameters'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(\"Saving model...\")\n",
        "\n",
        "if \"dp\" in config_args.keys() and eval(config_args['dp']) is True:\n",
        "    results_folder = f\"{config_args['results_folder']}/{id_reference}_dp\"\n",
        "else:\n",
        "    results_folder = f\"{config_args['results_folder']}/{id_reference}\"\n",
        "\n",
        "folder_name = \"\"\n",
        "\n",
        "if os.path.exists(os.path.join(config_args['results_folder'], id_reference, folder_name)):\n",
        "    shutil.rmtree(os.path.join(config_args['results_folder'], id_reference, folder_name))\n",
        "os.makedirs(os.path.join(config_args['results_folder'], id_reference, folder_name))\n",
        "\n",
        "torch.save(model.unet.state_dict(), f'{results_folder}/{folder_name}/unet')"
      ],
      "metadata": {
        "id": "KhnEBM3Ufjh4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}